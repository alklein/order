\section{The Calculus of Constructions}

The type system for caledon is based on the Calculus of Constructions as defined by Coquand \citep{coquand1986calculus}.
Since caledon might extend the calculus of constructions, it is important to view it as a pure type system. 
As Barendgregt points out, the common type theories can be recast as pure type systems
by choice of axioms. 

Jutting \citep{jutting1993typing} gave a proof that both typechecking and inference
in a pure type system with a finite axiom set is decidable.

Roorda \citep{roorda2001pure} gave an implementation of a functional programming language with 
pure type system and demonstrated its utility.

This is the pure type system where.

\begin{definition}
\textbf{(PTS for $CC$)}

\[
A = \{ *, \Box \}
\]

\[
S = \{ (* : \Box) \}
\]

\[ 
R = \{ (*,*,*),(*,\Box,\Box),(\Box,\Box,\Box),(\Box,*,*)\}
\]  
\label{coc:types}
\end{definition}

In this sytem, terms can depend on terms and types, 
and types can depend on types and terms.  

It has the well known strong normalization property, implying the termination of 
all lambda terms typable by $CC$ \citep{Geuvers94ashort} \citep{geuvers1991modular}.

It is necessary to be carefull with the types of equalities allowed in the conversion rule, 
as with more allowed equalities certain proofs
become significantly more complex.  For example, if $\eta$ reduction is permited, 
the church rosser theorem becomes no longer easily provable.

\begin{definition}
If $\Gamma \vdash_{CC} P : T : K$ means $\Gamma \vdash_{CC} P : T$ and $\Gamma \vdash_{CC} T : K$
\end{definition}


\subsection{Consistency of the Calculus of Constructions}

It is useful to first describe a few theorems and definitions relevant to $CC$.  

It is important to show that we can classify \citep{Geuvers94ashort} terms of $CC$.

\begin{definition}
Classifications:

\[
\m{Kind} := \{ A | \exists \Gamma . \Gamma \vdash_{CC} A : \Box \}
\]

\[
\m{Type} := \{ A | \exists \Gamma . \Gamma \vdash_{CC} A : * \}
\]

\[
\m{Constr} := \{ A | \exists T,\Gamma . \Gamma \vdash_{CC} A : T:\Box \}
\]

\[
\m{Obj} := \{ A | \exists T,\Gamma . \Gamma \vdash_{CC} A : T:* \}
\]

\end{definition}

\begin{theorem}
Classification:
$\m{Kind}\cap \m{Type} = \emptyset$ 
and
$\m{Constr}\cap \m{Obj} = \emptyset$ 
\end{theorem}

Write proof here.

As Geuvers points out, this can also proved using the Church-Rosser theorem, 
Subject Reduction and Uniqueness of Types theorem.  
The theorem defines an important technique for breaking down judgements into
different cases.

\begin{definition}
$ \m{Term}_{CC}  = \{ M : \exists T,\Gamma . \Gamma \vdash_{cc} M : T \}$
\end{definition}

\begin{theorem}
\textbf{(Strong Normalization)} $\forall M \in \m{Term}_{CC}. SN(M)$
\label{cc:cons}
\end{theorem}

The easiest to digest proof is also due to Geuvers \citep{Geuvers94ashort} 
but other proofs have also been proposed.  Geuvers' proof has the convenient
property that it does not depend too much on the definition of the set $SN$ 
(strongly normalizing). The only properties required are that $S \subseteq SN$ 
where $S$ is the set of sorts in the system. In the case of $CC$ $\Box,* \in SN$.
It also requires that $\Pi x : A . B \in SN$, and $\lambda x : A . B \in SN$ 
for any $A,B$.  Lastly it requires that saturated subsets of $SN$ are closed under
arbitrary intersection, and function space generation.

However, this proof is restricted to normalization in the calculus where only $\beta$ reduction 
is considered and not $\eta$ conversion.  The proof for the calculus with full reduction properties is
in Guevers' thesis \citep{geuvers1993logics}.  While evaluation of terms in Caledon without considering proof search does
not involve $\eta$ conversion, unification makes significant use of $\eta$ reduction and it is important to note
the consistency of the calculus of constructions even when $\eta$ reduction is considered.  
It is also important to note that in the Curry style calculus where types are omited from lambda abstractions, 
the church rosser theorem under $\eta$ conversion appears essentially for free\citep{miquel2001implicit}. Strong normalization 
follows, and by replacement of types, strong normalization follows for the Tarski style calculus.

\subsection{Impredicativity in the Calculus of Constructions}

It is also important to note that while the term language of the calculus of constructions is strongly normalizing, 
the predicates in the calculus of constructions are impredicative, meaning that small types can be generalized over all small types.
This adds yet another layer of computation into the Caledon language which might not terminate.  
Furthermore, the predicate language is insufficient to prove properties of larger types. 
Luo \citep{luo1989ecc} solves this by introducing universes into the Extended Calculus of Constructions.  
As defined by Luo, the Extended Calculus of Constructions is no longer a pure type system.

Harper \citep{harper1991type} provided an analysis of a few pure type systems with universes.
Agda, Idris, CoQ,
and Plastic \citep{callaghan2001implementation} all implement this teqchnique. 
Idris implementation uses a topological sort to allow for implicit universes
and thus does not allow the universes to become an annoyance in code. 
As Caledon is not intended to be a theorem prover, the impredicativity of the theorem layer is ignored and no 
predicative universe heirarchy is provided to construct metatheorems natively.

With the addition of inconsistent impredicative inductive types, it is possible to provide an implementation 
of the universe heirachy as a higher order datatype who's inhabitants are consistent.  This kind of implementation 
allows for metatheorem verification in a restricted subset of the language, albeit with the loss of 
simple cumulative universe inference.

\begin{lstlisting}
fixity pre 1 %* $\diamond$ *)
fixity lambda %* $\Pi$ *)
fixity lambda lam
unsound tm : {S : tm ty} tm S %* $\rightarrow$ *) prop
   | ty  = tm ty
   | %* $\diamond $ *)   = tm ty %* $\rightarrow$ *) tm ty
   | %* $\Pi$ *)   = [T : tm ty] (tm T %* $\rightarrow$ *) tm T) %* $\rightarrow$ *) tm $ %* $\diamond $ *) T
   | lam = [T : tm ty][F : tm T %* $\rightarrow$ *) tm T] tm {S = %* $\diamond$ *) T} (%* $\Pi$ *) A : T . F A)
   | raise = tm T %* $\rightarrow$ *) tm $ %* $\diamond$ *) T
\end{lstlisting}

\subsection{Theorems in Caledon}

It is important to note that in the programming language caledon, programs might not terminate.  
The search language itself is not consistent, and is not a theorem verification language like twelf.  
Rather, it is language for writing theorem proving programs.  

\begin{definition}
In the Caledon language, $\m{prop} = *$ and $\m{type} = \Box$
\end{definition}

It is important to note that in Caledon code, $\m{type}$ will never appear.  This is because $\m{type} : T$ for any $T$
is not provable in the Calculus of Constructions, and every term that appears in the language must have a provable type.

\begin{definition}
If $\Gamma \vdash_{CICC} P : T : \m{prop}$ in the caledon language, then $T$ is a theorem, and $P$ is a proof.
\end{definition}

Terms can be considered this way since the consistency of the calculus of constructions implies the strong normalization
of terms such as $P$.  No unbounded proof search is necessary to evaluate $P$.

When $CC$ was first developed, therems were proven and generated by explicitly defining
constructors and destructors for records and sum types.  Later, the inductive calculus of constructions was developed 
\citep{coquand1990inductively} which more accurately forms the basis of the CoQ programming language.  These types of inductive
constructions have been omited from Caledon to allow the addition of inductively defined, universally quantified predicates.
This omits the confusion that would be generated from having both inductively defined data and predicates in the system, as 
dependently typed logic programming treats predicates as both data and code.

Unfortunately, simple $CC$ does not make for a powerfull theorem proving language.  This can significantly 
limit the utility of powerfull theorem searching techniques.

The original calculus of constructions with a universe heirarchy included an impredicative type.

\begin{definition}
\textbf{(PTS for $CC_\omega$)}

\[
A = \{ \m{prop} \} \cup \{ \m{type}_i | i \in \mathbb{N} \}
\]

\[
S =   \{ (\m{prop} : \m{type}_0 ) \}
 \cup \{ (\m{type}_i : \m{type}_{i+1} ) | i \in \mathbb{N} \}
\]

\[ 
R = \{ (\m{type}_j,\m{type}_i,\m{type}_k) | j \leq k \wedge i \leq k \}
\cup \{ (\m{prop},t,t) | t \in A \}
\cup \{ (t,\m{prop},t) | t \in A \}
\]

\label{coc:utypes}
\end{definition}

While type checking with the addition of the impredicative universe is theoretically equivalently as difficult 
as without it, in practice it turns out to not be very usefull for proof or program writing, and 
languages like CoQ, Agda and Idris now omit it for the system with only predicative universes.

The addition of a universe heirachy into the Caledon Language is possible without too much added difficulty. 
Experimentation shows that the omitting the predicative $\m{prop}$ allows for a simpler implementation of type inference.
In this case, typechecking and unification are performed with the usually inconsistent assumption that
$\m{type} : \m{type}$, and a cycle checker is later applied to ensure that there is no instance of 
$\m{type}_i : \m{type}_i$ necessary.  

In the case where an impredicative universe is included, the extra axiom $\m{prop} : \m{type}$ would
need to be included, which would mean searching among two possibilities, rather than a single possibility 
when attempting to find a type for a metatheoretic hole.  The added nondeterminism tends to cause
an explosion in the time complexity of type inference.