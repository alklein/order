\section{Logic Programming}

Logic programming languages such as Prolog were originally designed as part of the AI
program, in much the same way Lisp was. Automated reasoning’s natural goal would
be to be able to arbitrarily prove theorems. A logic programming language would be
a set of axioms and a predicate, and if the predicate could be proven through those axioms,
the automated theorem prover would halt. These proof search procedures were
then constrained into useful programming semantics. When performed in a backtracking
manner, the process of proof search represented an formulation of procedural code
with powerful pattern matching. The Caledon language is a higher order backtracking
logic programming language in the style of Twelf. In this section I present some basic
intution for logic programming, rather than explaining it technically, and demonstrate the 
descriptive power of the system implemented in Caledon.

\FloatBarrier
\subsection{Basics}
We begin by defining addition on unary numbers in Caledon shown in shown in \ref{code:add}.

\begin{figure}[H]
\begin{lstlisting}
defn add : nat -> nat -> nat -> prop
  >| addZ = add zero A A
  >| addS = add (succ A) B (succ C) 
             <- add A B C
\end{lstlisting}
\caption{Addition in Caledon}
\label{code:add}
\end{figure}

One might notice that this definition is incredibly similar to its Haskell counterpart shown in \ref{code:hask}.

\begin{figure}[H]
\begin{lstlisting}
add :: nat -> nat -> nat
add Zero a = a
add (Succ a) b = Succ c
   where c = add a b
\end{lstlisting}
\caption{Addition in Haskell}
\label{code:hask}
\end{figure}

We can read the logic programming definition as we would read the functional definition with pattern
matching, knowing that an intelligent compiler would be able to convert the first into the second.  
search allows one to define essentially nondeterministic programs. A common use for
logic programming has been to search for solutions to combinatorial games such as tic-tac-toe, 
without the programmer worrying about the order of the search. As this tends
to produce ineficient code, this use style is discouraged. Rather, a more procedural view of logic programming is encouraged where pattern match and search is performed
in the order it appears.

\begin{figure}[H]
\begin{lstlisting}
defn p : T_1 -> ... -> T_r -> prop
  >| n1 = p T_1 ... T_r <- p_1,1 ... <- p_1,k_1
...
  >| nN = p T_1 ... T_r <- p_n,1 ... <- p_n,k_n

query prg = p t1 ... tr
\end{lstlisting}
\caption{Format of a Caledon Logic Program}
\label{code:format}
\end{figure}

In this view, a program of the form \ref{code:format}
should be considered a program which first attempts to prove using axiom n1 by matching prg with ``p T1 ... Tr'' and then
attempting to prove $p_{1,1}$ and so on.


\FloatBarrier
\subsection{Higher Order Logic Programming}

Just as imperative programs benefit from the addition of higher order functions, logic programs benefit from the addition of both
higher order predicates and patterns.  

A common request by functional programming language users is that they would like to be able to abstract patterns even more than just over
arguments.  

\begin{figure}[H]
\begin{lstlisting}
func (Var a) = code1
func (Forall var val) = Exists var code
func (Exists var val) = Forall var code
func (And a b) = code2
\end{lstlisting}
\caption{Unecessarily verbose code}
\label{code:verbose}
\end{figure}

A serious amount of code is repeated in lines 2 and 3 of example \ref{code:verbose}, 
but in common languages it is impossible to simplify this.
What a programmer would actually like to express is shown in \ref{code:Fideal}.

\begin{figure}[H]
\begin{lstlisting}
func (Var a) = code1
func (f var val) = f var code
func (And a b) = code2
\end{lstlisting}
\caption{Ideal code. $f$ is a constructor in strong head normal form.}
\label{code:Fideal}
\end{figure}

In higher order logic programming languages like $\lambda$Prolog, this sort of simplification 
is in fact possible to express as shown in \ref{code:lprolog}.

\begin{figure}[H]
\begin{lstlisting}
defn func : term -> term -> prop 
  | f1 = func (var A) R <- [code1]
  | f2 = func (F Var Val) (f Var R) <- [code]
  | f3 = func (and A B) R <- [code2]
\end{lstlisting}
\caption{Expressing constructor variables in patterns}
\label{code:lprolog}
\end{figure}

\FloatBarrier
\subsection{Higher Order Programming}

Fortunately, higher order functions need not be restricted to patterns.  Macros provide even more ways to generalize code. 

A great example is the function application operator from Haskell.  
We can define this in Caledon as shown in \ref{code:macros}

\begin{figure}[H]
\begin{lstlisting}
fixity right 0 @
defn @ : (At -> Bt) -> At -> Bt
  as ?\ At Bt . \ f : At -> Bt . \ a : At . f a

\end{lstlisting}
\caption{Definitions for expressive syntax}
\label{code:macros}
\end{figure}

In many cases, allowing these definitions allows for significant simplification of syntax.
The reader familiar with languages like Twelf, Haskell, and Agda might notice the 
implicit abstraction of the type variables At and Bt in the type of @ in \ref{code:macros}. The rest of this
paper is concerned with formalizing these implicit abstractions and letting them have
as much power as possible. For example, one might make these abstractions explicit by
instead declaring at the beginning \ref{code:expHask}

\begin{figure}[H]
\begin{lstlisting}
infixr 0 @
(@) :: forall At Bt . (At -> Bt) -> At -> Bt
(@) = \ f . \ a . f a
\end{lstlisting}
\caption{Explicit Haskell style abstractions}
\label{code:expHask}
\end{figure}

However, in a dependently typed language, every function type is a dependent product
(forall).  This makes it necessary to provide a new (explicit) implicit dependent product - $?\foral$ or $?\Pi$.


\begin{figure}[H]
\begin{lstlisting}
fixity right 0 @
defn @ : {At Bt:prop} (At -> Bt) -> At -> Bt
  as ?\ At Bt . \ f . \ a . f a

\end{lstlisting}
\caption{The (explicit) implicit equivalent of \ref{code:macros}}
\label{code:expimp}
\end{figure}

Haskell also has type classes. For example, the type of “show” can be seen in \ref{code:showty}.

\begin{figure}[H]
\begin{lstlisting}
show :: Show a => a -> String
\end{lstlisting}
\caption{The type of show}
\label{code:showty}
\end{figure}

In Caledon, these can be written similarly as in \ref{code:cshowty}

\begin{figure}[H]
\begin{lstlisting}
defn show : showC A => A -> string
defn show : {unused : showC A } A -> string
\end{lstlisting}
\caption{Equivalent types for show in Caledon}
\label{code:cshowty}
\end{figure}

However, since implicit arguments are a natural extension of the dependent type
system in Caledon, no restrictions are made on the number of arguments, or difficulty
of computing. Unfortunately, since computation is primarily accomplished by the logic
programming fragment of the language rather than the functional fragment of the language,
the correspondence between these programmable implicit arguments and type
classes is not one to one. It is possible to replicate virtually all of the functionality of
type classes in the implicit argument system, but the syntax required to do so can become
verbose. Rather than attempting to simulate type classes, more creative uses are
possible, such as computing the symbolic derivative of a type for use in a (albeit, slow
and unnecessary) generic zipper library, or writing programs that compile differently
with different types in different environments.
